{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40746772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow.keras.backend \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.optimizers import SGD\n",
    "from keras import Sequential\n",
    "\n",
    "import math,time,random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./rawdata/BPI2016_Clicks_Logged_In.csv', sep=';', encoding='latin-1', keep_default_na=False)\n",
    "\n",
    "\n",
    "\n",
    "df['time'] = pd.to_datetime(df['TIMESTAMP'])\n",
    "df['dates'] = df['time'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wrkmsg=pd.read_csv('./rawdata/BPI2016_Werkmap_Messages.csv', encoding='latin-1', sep=';', keep_default_na=False)\n",
    "wm_columns_to_keep = ['EventDateTime','CustomerID']\n",
    "\n",
    "wrkmsg=wrkmsg[[c for c in wrkmsg.columns if c in wm_columns_to_keep]]\n",
    "wrkmsg.columns = ['CustomerID','ContactDate']\n",
    "\n",
    "wrkmsg['ContactDate'] = pd.to_datetime(wrkmsg['ContactDate'])\n",
    "wrkmsg['ContactTimeStart'] = [dt.datetime.time(d) for d in wrkmsg['ContactDate']] \n",
    "wrkmsg['ContactDate'] = [dt.datetime.date(d) for d in wrkmsg['ContactDate']] \n",
    "wrkmsg.head()\n",
    "wrkmsg_customers=(df[df['CustomerID'].isin(wrkmsg['CustomerID'].unique())])\n",
    "wrkmsg_customers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f87b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkmsg['dates']=wrkmsg['ContactDate']\n",
    "wrkmsg_customers_we=wrkmsg_customers[wrkmsg_customers['page_load_error']==0]\n",
    "# session_interaction = pd.merge(wrkmsg_customers_we, wrkmsg, how=\"left\", on=[\"CustomerID\", \"dates\"])\n",
    "# session_interaction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78611ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkmsg_customers_we.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ea862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique traces count\", len(df.SessionID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f27d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique events count\",len(df.PAGE_NAME.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1493ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total events count\",len(df.PAGE_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf8f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df.PAGE_NAME)/len(df.SessionID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrkmp_custdates=(wrkmsg.groupby(['CustomerID','dates'])).agg(count=('ContactTimeStart', 'count')).reset_index()\n",
    "session_wrk = pd.merge(wrkmsg_customers_we, wrkmp_custdates, how=\"left\", on=[\"CustomerID\", \"dates\"])\n",
    "session_wrk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f185fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_wrk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a35f2e",
   "metadata": {},
   "source": [
    "#TO BE ADDED AS A LAYER?\n",
    "cust_info = session_wrk.loc[:,['CustomerID','AgeCategory','Gender','Office_U','Office_W']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_wrk = session_wrk.drop(['SessionID','AgeCategory','Gender','Office_U','Office_W','page_load_error'], axis=1)\n",
    "session_wrk = session_wrk.drop(['Office_U','Office_W','page_load_error'], axis=1)\n",
    "session_wrk.loc[:,'count']=session_wrk.loc[:,'count'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b580852",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_wrk['Flag'] = np.where(session_wrk['count']>0, 1, 0)\n",
    "cust_ids = session_wrk['CustomerID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_wrk[session_wrk.page_action_detail!='']['SessionID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = session_wrk['PAGE_NAME'].value_counts()\n",
    "# print (counts[:])\n",
    "thres=session_wrk.shape[0]*0.005\n",
    "session_wrk_fil=session_wrk[session_wrk['PAGE_NAME'].isin(counts[counts > thres].index)]\n",
    "print(len(session_wrk_fil.PAGE_NAME.unique()))\n",
    "counts1 = session_wrk_fil['PAGE_NAME'].value_counts()\n",
    "print (counts1[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836140f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(session_wrk[session_wrk.SessionID==44265671].groupby(['SessionID'])).agg(page_count=('PAGE_NAME', 'count'), flag=('Flag','max')).reset_index()\n",
    "# ['page_action_detail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561212bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(session_wrk_fil[session_wrk_fil.SessionID==44265671].groupby(['SessionID'])).agg(page_count=('PAGE_NAME', 'count'), flag=('Flag','max')).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370985ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_wrk_fil_pc_s=(session_wrk_fil.groupby(['SessionID'])).agg(page_count=('PAGE_NAME', 'nunique')).reset_index()\n",
    "# session_wrk_fil_pc=(session_wrk_fil_pc_s.groupby(['page_count'])).agg(num_sessions=('SessionID', 'nunique')).reset_index()\n",
    "\n",
    "\n",
    "session_wrk_fil_pc_s=(session_wrk_fil.groupby(['SessionID'])).agg(page_count=('PAGE_NAME', 'count'), flag=('Flag','max')).reset_index()\n",
    "session_wrk_fil_pc=(session_wrk_fil_pc_s.groupby(['page_count','flag'])).agg(num_sessions=('SessionID', 'count')).reset_index()\n",
    "tot_sess = len(session_wrk_fil['SessionID'].unique())\n",
    "session_wrk_fil_pc['perc_sess']=session_wrk_fil_pc['num_sessions']/tot_sess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ead2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_wrk_fil_pc_s[session_wrk_fil_pc_s.SessionID==44265671]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e59428",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_wrk_fil_pc.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback=7\n",
    "# min_los = (lookback/2)\n",
    "min_los = 3\n",
    "sessions_to_keep = session_wrk_fil_pc_s[(session_wrk_fil_pc_s['page_count']>=min_los) & (session_wrk_fil_pc_s['page_count']<=lookback)]['SessionID']\n",
    "# sessions_to_keep = session_wrk_fil_pc_s[(session_wrk_fil_pc_s['page_count']>=3 & (session_wrk_fil_pc_s['page_count']<=8))]['SessionID']\n",
    "# sessions_to_keep = session_wrk_fil_pc_s[(session_wrk_fil_pc_s['page_count']==lookback)]['SessionID']\n",
    "\n",
    "session_wrk_lkbk=session_wrk_fil[session_wrk_fil['SessionID'].isin(sessions_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_wrk_lkbk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd31f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_wrk_lkbk.shape\n",
    "sessions_to_keep.shape\n",
    "session_wrk_lkbk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb40a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_wrk_lkbk['page_action_detail'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc019a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_wrk_lkbk[session_wrk_lkbk['SessionID']==44265671]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2700be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_wrk_fil.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa9c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_wrk_fil.loc[:,'dates'] =  session_wrk_fil.loc[:,'dates'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013199ae",
   "metadata": {},
   "source": [
    "encoder = LabelEncoder() #Label encoder may introduce order; One-HOt encodingg!!\n",
    "\n",
    "session_wrk_fil.loc[:,'CustomerID'] =  encoder.fit_transform(session_wrk_fil.loc[:,'CustomerID'])\n",
    "# session_wrk_fil.loc[:,'TIMESTAMP'] =  encoder.fit_transform(session_wrk_fil.loc[:,'TIMESTAMP'])\n",
    "session_wrk_fil.loc[:,'PAGE_NAME'] =  encoder.fit_transform(session_wrk_fil.loc[:,'PAGE_NAME'])\n",
    "session_wrk_fil.loc[:,'VHOST'] =  encoder.fit_transform(session_wrk_fil.loc[:,'VHOST'])\n",
    "session_wrk_fil.loc[:,'URL_FILE'] =  encoder.fit_transform(session_wrk_fil.loc[:,'URL_FILE'])\n",
    "session_wrk_fil.loc[:,'xps_info'] =  encoder.fit_transform(session_wrk_fil.loc[:,'xps_info'])\n",
    "session_wrk_fil.loc[:,'dates'] =  encoder.fit_transform(session_wrk_fil.loc[:,'dates'])\n",
    "session_wrk_fil.loc[:,'REF_URL_category'] =  encoder.fit_transform(session_wrk_fil.loc[:,'REF_URL_category'])\n",
    "session_wrk_fil.loc[:,'page_action_detail_EN'] =  encoder.fit_transform(session_wrk_fil.loc[:,'page_action_detail_EN'])\n",
    "session_wrk_fil.loc[:,'service_detail_EN'] =  encoder.fit_transform(session_wrk_fil.loc[:,'service_detail_EN'])\n",
    "session_wrk_fil.loc[:,'tip_EN'] =  encoder.fit_transform(session_wrk_fil.loc[:,'tip_EN'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b1e4e",
   "metadata": {},
   "source": [
    "data = session_wrk_fil.copy()\n",
    "\n",
    "categorical_cols = [ 'PAGE_NAME'] \n",
    "cols_to_drop = ['IPID','tip','service_detail','page_action_detail','count','dates','xps_info', 'VHOST', 'URL_FILE' ,'page_action_detail_EN', 'tip_EN', 'REF_URL_category', 'service_detail_EN']\n",
    "\n",
    "data = data.drop(cols_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d931e9",
   "metadata": {},
   "source": [
    "data.sort_values(by=['CustomerID','TIMESTAMP'],ascending=False,inplace=True)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d63699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_timespent(df):\n",
    "    return_df = pd.DataFrame(columns = df.columns)\n",
    "#     return_df=session_interaction\n",
    "    unique_sess =  df['SessionID'].unique()\n",
    "    for sess in unique_sess:\n",
    "        session_level = df.loc[df['SessionID']==sess]\n",
    "        session_level = df.sort_values(by=['SessionID','TIMESTAMP'])\n",
    "        session_level.loc[:,'timespent']=session_level.loc[:,'time'].diff().apply(lambda x: x/np.timedelta64(1, 's')).fillna(0).astype('int64')\n",
    "        return_df=return_df.append(session_level,ignore_index=True)\n",
    "#         print(return_df.head())\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = session_wrk_lkbk.copy()\n",
    "\n",
    "categorical_cols = [ 'PAGE_NAME', 'AgeCategory', 'Gender'] \n",
    "cols_to_drop = ['page_action_detail','IPID','tip','service_detail','page_action_detail_EN','count','dates','xps_info', 'VHOST', 'URL_FILE' , 'tip_EN', 'REF_URL_category', 'service_detail_EN']\n",
    "\n",
    "data = data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "data.sort_values(by=['CustomerID','TIMESTAMP'],ascending=False,inplace=True)\n",
    "\n",
    "data.loc[:,'CustomerID'] =  session_wrk_fil.loc[:,'CustomerID'].astype(str)\n",
    "\n",
    "data.loc[:,'TIMESTAMP']=(data['time']).astype(int)/ 10**9\n",
    "\n",
    "\n",
    "# data.loc[:,'TIMESTAMP']= data['TIMESTAMP'].transform( lambda x: x-x.min())\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# data.loc[:,'TIMESTAMP'] =  scaler.fit_transform(data[['TIMESTAMP']])\n",
    "\n",
    "\n",
    "\n",
    "encoder = LabelEncoder() #Label encoder may introduce order; One-HOt encodingg!!\n",
    "\n",
    "# data.loc[:,'CustomerID'] =  encoder.fit_transform(data.loc[:,'CustomerID'])\n",
    "\n",
    "data = data.drop(['CustomerID'], axis=1)\n",
    "\n",
    "# data = add_timespent(data)\n",
    "\n",
    "data = data.drop(['time'], axis=1)\n",
    "\n",
    "encoded_data = pd.get_dummies(data, columns = categorical_cols) #TO TRY - binary encoding or other encodings\n",
    "\n",
    "#Customer visit features - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7509ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_wrk_lkbk['page_action_detail'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(session_wrk_lkbk.SessionID.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11727df2",
   "metadata": {
    "tags": []
   },
   "source": [
    "session_wrk_fil['TIMESTAMP'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TIMESTAMP'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe0f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balancer(d):\n",
    "    sess_K1 = d[d.Flag == 1]['SessionID']\n",
    "    sess_K0 = d[d.Flag == 0]['SessionID'].unique()\n",
    "    print(len(sess_K0))\n",
    "    print(len(sess_K0))\n",
    "    smsk = np.random.rand(len(sess_K0)) > 0.81\n",
    "    sess2keep = sess_K0[smsk]\n",
    "    print(len(sess2keep))\n",
    "    fl = np.append(sess2keep,sess_K1)\n",
    "#     sess2keep = sess2keep.append(sess_K1)\n",
    "    \n",
    "    return d[d.SessionID.isin(fl)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 way split\n",
    "cust_ids = encoded_data['SessionID'].unique()\n",
    "\n",
    "msk = np.random.rand(len(cust_ids)) < 0.5\n",
    "train_ids = cust_ids[msk]\n",
    "val_test_ids = cust_ids[~msk]\n",
    "\n",
    "train = encoded_data[encoded_data['SessionID'].isin(train_ids)]\n",
    "\n",
    "msk = np.random.rand(len(val_test_ids)) < 0.5\n",
    "validate_ids = val_test_ids[msk]\n",
    "test_ids = val_test_ids[~msk]\n",
    "\n",
    "validate = encoded_data[encoded_data['SessionID'].isin(validate_ids)]\n",
    "test = encoded_data[encoded_data['SessionID'].isin(test_ids)]\n",
    "\n",
    "\n",
    "# train = class_balancer(train)\n",
    "# validate = class_balancer(validate)\n",
    "# test = class_balancer(test)\n",
    "\n",
    "\n",
    "train_target = train.loc[:,\"Flag\"]\n",
    "validate_target = validate.loc[:,\"Flag\"]\n",
    "test_target = test.loc[:,\"Flag\"]\n",
    "\n",
    "print(len(train[train.Flag==1]['SessionID'].unique())/len(train['SessionID'].unique())) #Sessions with flag =1 \n",
    "print(len(validate[validate.Flag==1]['SessionID'].unique())/len(validate['SessionID'].unique())) #Sessions with flag =1 \n",
    "print(len(test[test.Flag==1]['SessionID'].unique())/len(test['SessionID'].unique())) #Sessions with flag =1 \n",
    "\n",
    "# train_data = train.drop(['Flag','SessionID'], axis=1)\n",
    "# validate_data = validate.drop(['Flag','SessionID'], axis=1)\n",
    "# test_data = test.drop(['Flag','SessionID'], axis=1)\n",
    "train_data = train.drop(['Flag'], axis=1)\n",
    "validate_data = validate.drop(['Flag'], axis=1)\n",
    "test_data = test.drop(['Flag'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319fdfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset sesss with KPI =1\n",
    "# from remaining sample!!\n",
    "# d = validate\n",
    "validate_ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33cafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d8b62c",
   "metadata": {},
   "source": [
    "print(len(validate[validate.Flag==1]['SessionID'].unique())/len(validate['SessionID'].unique())) #Sessions with flag =1 \n",
    "\n",
    "\n",
    "a=class_balancer(validate)\n",
    "len(a.SessionID.unique())\n",
    "print(len(a[a.Flag==1]['SessionID'].unique())/len(a['SessionID'].unique())) #Sessions with flag =1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_data(data):\n",
    "#     data=data.drop('Flag', axis=1)\n",
    "    data=data.sort_values('SessionID')\n",
    "    ndata = data.copy()\n",
    "    unique_sess = data['SessionID'].unique()\n",
    "    print(len(unique_sess))\n",
    "#     x = np.empty(data.shape[0])\n",
    "    for id in unique_sess:\n",
    "        sub_data = data.loc[data.SessionID == id,:]\n",
    "        \n",
    "        sub_data.loc[:,'TIMESTAMP'] = sub_data.loc[:,'TIMESTAMP'].transform( lambda x: x-x.min())\n",
    "        ndata.loc[ndata.SessionID == id,:] = sub_data.copy()\n",
    "        \n",
    "    cols = ndata.columns[2:]\n",
    "#     print(cols)\n",
    "    print(ndata.columns)\n",
    "    s1 = ndata[ndata.columns[:2]]\n",
    "    s2 = ndata[cols].idxmax(axis=1)\n",
    "    nd = pd.concat([s1,s2],axis=1,keys=None)\n",
    "    print(nd)\n",
    "    nd.rename(columns={ nd.columns[2]: \"PAGE_NAME\" }, inplace = True)\n",
    "    session_level = nd.pivot_table(index='SessionID', columns='PAGE_NAME', values=['TIMESTAMP']).reset_index()\n",
    "#         rdf=pd.concat(rdf,session_level)\n",
    "    return session_level.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e69658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pivot_data(test_data.head(20))\n",
    "# test.head(10)[test.columns[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0cfd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c640b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(data, lookback, nsamples):\n",
    "    data=data.sort_values(['SessionID','TIMESTAMP'])\n",
    "    unique_sess = data.SessionID.unique()\n",
    "    rdf=[]\n",
    "    j=len(data.columns)\n",
    "    k=lookback\n",
    "    x = np.empty((nsamples,j,k))\n",
    "    i=0\n",
    "    for id in unique_sess:\n",
    "        sub_data = data[data.SessionID == id]\n",
    "#         sub_data = sub_data.drop('SessionID', axis=1)\n",
    "        sub_data = sub_data.drop(['SessionID','TIMESTAMP'], axis=1)\n",
    "#         sub_data.loc[:,'TIMESTAMP']= sub_data['TIMESTAMP'].transform( lambda x: (x-x.min()/(x.max())))\n",
    "#         sub_data.loc[:,'TIMESTAMP']= sub_data['TIMESTAMP'].transform( lambda x: x-x.min())\n",
    "        temp2d = np.transpose(keras.preprocessing.sequence.pad_sequences(np.transpose(sub_data.values), maxlen=lookback, dtype='float32', value=0., padding='pre'))\n",
    "        rdf.append(temp2d)\n",
    "        i += 1\n",
    "    return np.stack(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f7f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nsamples = len(train_data.SessionID.unique())\n",
    "test_nsamples = len(test_data.SessionID.unique())\n",
    "validate_nsamples = len(validate_data.SessionID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.sort_values('SessionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dad8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rsd = reshape_data(train_data, lookback, train_nsamples)\n",
    "test_rsd = reshape_data(test_data, lookback, test_nsamples)\n",
    "validate_rsd = reshape_data(validate_data, lookback, validate_nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_rsd.shape)\n",
    "print(test_rsd.shape)\n",
    "print(validate_rsd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a040bcc",
   "metadata": {},
   "source": [
    "train_rsd = np.take(train_rsd, dummy_idx_to_keep, axis = 0)\n",
    "train_target = np.take(train_target, dummy_idx_to_keep, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49ffa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_keep = np.unique(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f750b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_to_keep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (cases_to_keep[0]).extend(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(cases_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_to_keep[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df19c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balancer(d):\n",
    "    sess_K1 = d[d.Flag == 1]['SessionID']\n",
    "    sess_K0 = d[d.Flag == 0]['SessionID'].unique()\n",
    "    print(len(sess_K0))\n",
    "    print(len(sess_K0))\n",
    "    smsk = np.random.rand(len(sess_K0)) > 0.81\n",
    "    sess2keep = sess_K0[smsk]\n",
    "    print(len(sess2keep))\n",
    "    fl = np.append(sess2keep,sess_K1)\n",
    "#     sess2keep = sess2keep.append(sess_K1)\n",
    "    \n",
    "    return d[d.SessionID.isin(fl)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390075f1",
   "metadata": {},
   "source": [
    "import time\n",
    "st = time.time()\n",
    "\n",
    "print(validate_rsd.shape, test_rsd[0:10].shape)\n",
    "\n",
    "a = torch.flatten(torch.tensor(test_rsd[0:10]), start_dim=1)  #simulates running traces\n",
    "b = torch.flatten(torch.tensor(validate_rsd), start_dim=1) #dataset to find candidates from, also used for tuning the model \n",
    "\n",
    "print(a.shape, b.shape)\n",
    "\n",
    "\n",
    "# print(torch.cdist(a, b).topk(100, largest=False)[1][0:10])\n",
    "\n",
    "print( time.time() - st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7e206",
   "metadata": {
    "tags": []
   },
   "source": [
    "train_nsamples = int((train_data.shape[0])/lookback)\n",
    "test_nsamples = int((test_data.shape[0])/lookback)\n",
    "validate_nsamples = int((validate_data.shape[0])/lookback)\n",
    "\n",
    "# train_data= create_data_set(train_data,lookback)\n",
    "# train_data.shape\n",
    "train_data = np.reshape(train_data.values,(train_nsamples, lookback, train_data.shape[1]))\n",
    "validate_data = np.reshape(validate_data.values,(validate_nsamples, lookback, validate_data.shape[1]))\n",
    "test_data = np.reshape(test_data.values,(test_nsamples, lookback, test_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e51064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_target.shape\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63f446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape_target(data, lookback, nsamples):\n",
    "    unique_sess = data.SessionID.unique()\n",
    "    y = []\n",
    "    y=(data.groupby(['SessionID'])).agg(KPI=('Flag', 'max')).reset_index()\n",
    "#     for id in unique_sess:\n",
    "#         f=data[data.SessionID == id]['Flag']\n",
    "#         y = np.append(y,f)\n",
    "    y['SessionID'] = pd.Categorical(y['SessionID'], \n",
    "                                    categories=unique_sess, \n",
    "                                    ordered=True)\n",
    "    y.sort_values('SessionID')\n",
    "#     print(data.SessionID)\n",
    "#     print(unique_sess)\n",
    "#     print(y)\n",
    "    return y.KPI.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = reshape_target(train, lookback, train_nsamples)\n",
    "validate_target = reshape_target(validate, lookback, validate_nsamples)\n",
    "test_target = reshape_target(test, lookback, test_nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948049d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_target.shape, train_rsd.shape)\n",
    "# print(validate_target.shape)\n",
    "# print(test_target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6dcb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###IMPLEMENT DISTANCE BASED CLASS BALANCER HERE\n",
    "import torch\n",
    "\n",
    "def dist_class_balancer(dummy_d,dummy_t, k):\n",
    "\n",
    "    stime=time.time()\n",
    "    # dummy_d = train_rsd[0:100]\n",
    "    # dummy_t = train_target[0:100]\n",
    "#     dummy_d = train_rsd\n",
    "#     dummy_t = train_target\n",
    "\n",
    "    larger_sub_idx = np.where(dummy_t == 0)\n",
    "    smaller_sub_idx = np.where(dummy_t != 0)\n",
    "\n",
    "    cases_to_keep = smaller_sub_idx\n",
    "\n",
    "    larger_sub = np.take(dummy_d, larger_sub_idx, axis=0)\n",
    "    smaller_sub = np.take(dummy_d, smaller_sub_idx, axis=0)\n",
    "\n",
    "    # np.squeeze(smaller_sub, axis=(2,)).shape\n",
    "\n",
    "    # print(dummy_d.shape, larger_sub.shape, smaller_sub.shape)\n",
    "\n",
    "    # print(np.squeeze(smaller_sub, axis=(0,)).shape)\n",
    "\n",
    "    smaller_torch = torch.flatten(torch.tensor(np.squeeze(smaller_sub, axis=(0,))), start_dim=1)\n",
    "    larger_torch = torch.flatten(torch.tensor(np.squeeze(larger_sub, axis=(0,))), start_dim=1)\n",
    "\n",
    "\n",
    "    print(smaller_torch.shape, larger_torch.shape)\n",
    "    # print(torch.cdist(smaller_torch, larger_torch).topk(10, largest=False))\n",
    "#     print(len(set(torch.flatten(torch.cdist(smaller_torch, larger_torch).topk(5, largest=False)[1]).detach().numpy())))\n",
    "\n",
    "    to_keep = np.unique(torch.flatten(torch.cdist(smaller_torch, larger_torch).topk(k, largest=False)[1]).detach().numpy())\n",
    "    dummy_idx_to_keep = np.concatenate((cases_to_keep[0],to_keep))\n",
    "    # cases_to_keep.append()\n",
    "\n",
    "    print(time.time()-stime)\n",
    "    return( np.unique(dummy_idx_to_keep))\n",
    "#     return (np.take(dummy_d, dummy_idx_to_keep, axis = 0), np.take(dummy_t, dummy_idx_to_keep, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = dist_class_balancer(train_rsd, train_target, 5)\n",
    "\n",
    "train_ids.sort()\n",
    "bal_train_ids = np.take(np.squeeze(train_ids), train_idx, axis = 0)\n",
    "bal_train_rsd = np.take(train_rsd, train_idx, axis = 0)\n",
    "bal_train_target = np.take(train_target, train_idx, axis = 0)\n",
    "\n",
    "test_idx = dist_class_balancer(test_rsd, test_target, 5)\n",
    "\n",
    "test_ids.sort()\n",
    "bal_test_ids = np.take(np.squeeze(test_ids), test_idx, axis = 0)\n",
    "bal_test_rsd = np.take(test_rsd, test_idx, axis = 0)\n",
    "bal_test_target = np.take(test_target, test_idx, axis = 0)\n",
    "\n",
    "validate_idx = dist_class_balancer(validate_rsd, validate_target, 5)\n",
    "validate_ids.sort()\n",
    "bal_validate_ids = np.take(np.squeeze(validate_ids), validate_idx, axis = 0)\n",
    "bal_validate_rsd = np.take(validate_rsd, validate_idx, axis = 0)\n",
    "bal_validate_target = np.take(validate_target, validate_idx, axis = 0)\n",
    "\n",
    "# train_rsd, train_target = dist_class_balancer(train_rsd, train_target, 5)\n",
    "# test_rsd, test_target = dist_class_balancer(test_rsd, test_target, 5)\n",
    "# validate_rsd, validate_target = dist_class_balancer(validate_rsd, validate_target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b8b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rsd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class timeseries(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y,dtype=torch.float32)\n",
    "        self.len = x.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_me(x,y):\n",
    "    test_target = x\n",
    "#     print(y)\n",
    "    pred_test_y = (y.detach().numpy() > 0.).astype(int)\n",
    "\n",
    "    matrix = metrics.confusion_matrix(test_target, pred_test_y ) \n",
    "    return(\"F1:\",metrics.f1_score(test_target, pred_test_y ),\"Accuracy:\",metrics.accuracy_score(test_target, pred_test_y ),\"ROC_AUC:\",metrics.roc_auc_score(test_target, y.detach().numpy() )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4136cdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c04c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "number_of_features = train_rsd.shape[2]\n",
    "\n",
    "hidden_layers = 32\n",
    "\n",
    "class LSTM_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_nn,self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(lookback)\n",
    "        self.do = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size=number_of_features,hidden_size=hidden_layers,num_layers=1,batch_first=True,bidirectional=True,dropout=0.0)\n",
    "#         self.fc1 = nn.Linear(in_features=hidden_layers,out_features=1)\n",
    "        self.fc1 = nn.Linear(in_features=hidden_layers*2,out_features=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.do(x)\n",
    "        output = self.bn(output)\n",
    "#         output = self.bn(x)\n",
    "#         output = output[:,-1,:]\n",
    "        output,_status = self.lstm(output)\n",
    "#         print(output.size())\n",
    "        output = output[:,-1,:]\n",
    "#         print(output.size())\n",
    "        output = self.do(output)\n",
    "        output = self.fc1(torch.relu(output))\n",
    "#         output = torch.relu(self.fc1(output))\n",
    "        return output\n",
    "\n",
    "model = LSTM_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1, requires_grad=True)\n",
    "        nn.init.kaiming_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim, requires_grad=True))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim \n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        \n",
    "        return (torch.sum(weighted_input, 1), a)\n",
    "#         return weighted_input\n",
    "#         if self.training:\n",
    "#             return torch.sum(weighted_input, 1)\n",
    "#         else:\n",
    "#             return (torch.sum(weighted_input, 1), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4e0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "number_of_features = train_rsd.shape[2]\n",
    "\n",
    "hidden_layers = 96\n",
    "\n",
    "class LSTMAttn_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMAttn_nn,self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(lookback)\n",
    "        self.do = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size=number_of_features,hidden_size=hidden_layers,num_layers=1,batch_first=True,bidirectional=True,dropout=0.0)\n",
    "#         self.fc1 = nn.Linear(in_features=hidden_layers,out_features=1)\n",
    "        self.fc1 = nn.Linear(in_features=hidden_layers*2,out_features=1)\n",
    "        self.attn = nn.Linear(hidden_layers , hidden_layers)\n",
    "        self.attn_combine = nn.Linear(hidden_layers * 2, hidden_layers)\n",
    "#         self.attention = Attention(hidden_layers,lookback)\n",
    "        self.attention = Attention(hidden_layers* 2,lookback )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.do(x)\n",
    "        bn_output = self.bn(output)\n",
    "#         output = self.bn(x)\n",
    "#         output = output[:,-1,:]\n",
    "        lstm_output, _status = self.lstm(bn_output)\n",
    "\n",
    "        lstm_attn, attn_wts  = self.attention(lstm_output)\n",
    "        lstm_attn_do = self.do(lstm_attn)\n",
    "#         relu_output = self.fc1(torch.relu(lstm_attn_do))\n",
    "        relu_output = torch.relu(self.fc1(lstm_attn_do))\n",
    "        return(relu_output, attn_wts)\n",
    "#         if self.training:\n",
    "#             return relu_output\n",
    "#         else:\n",
    "#             return(relu_output, attn_wts)\n",
    "\n",
    "model = LSTMAttn_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78122916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "learning_rate=0.0005\n",
    "\n",
    "epochs = 100\n",
    "batch_size=13\n",
    "\n",
    "# class_weight = torch.tensor([1., 5.])\n",
    "\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# criterion = torch.nn.MSELoss()\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(),lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,weight_decay=1e-3)\n",
    "\n",
    "\n",
    "# dataset = timeseries(train_rsd[0:5000],train_target[0:5000])\n",
    "dataset = timeseries( bal_train_rsd, bal_train_target)\n",
    "\n",
    "validate_set = timeseries( bal_validate_rsd, bal_validate_target)\n",
    "# validate_set = timeseries(validate_rsd[1000:2000],validate_target[1000:2000])\n",
    "\n",
    "#dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader \n",
    "train_loader = DataLoader(dataset,shuffle=True,batch_size=batch_size)\n",
    "validate_loader = DataLoader(validate_set,shuffle=True,batch_size=batch_size)\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "train_total_attn = np.zeros((1,lookback))\n",
    "\n",
    "for i in range(epochs):\n",
    "    avg_loss = 0\n",
    "    train_pred = torch.zeros(len(train_loader))\n",
    "    train_tar = torch.zeros(len(train_loader))\n",
    "    for j,data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "#         print(data[0][0].view(-1,5,16))\n",
    "        model_ot = model(data[:][0].view(-1,lookback,number_of_features))\n",
    "        y_pred = model_ot[0].reshape(-1)\n",
    "        \n",
    "        attn = model_ot[1]\n",
    "#         print(attn.size())\n",
    "        train_total_attn = np.vstack((train_total_attn,attn.detach().numpy()))\n",
    "        loss = criterion(y_pred,data[:][1])\n",
    "        loss.backward()\n",
    "        clipping_value = 1.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += loss.item() #/ len(train_loader)\n",
    "        train_pred = torch.cat((train_pred, y_pred), 0)\n",
    "        train_tar = torch.cat((train_tar, data[:][1]), 0)\n",
    "        \n",
    "    if (i+1)%25 == 0 or i==0:\n",
    "        print(i,\"th epoch : \")\n",
    "#         print(sum((train_tar.detach().numpy()==adj_train_pred))/len(adj_train_pred))\n",
    "        print(\"Training metrics: Loss\",avg_loss, validate_me(train_tar,train_pred))\n",
    "        total_attn = np.zeros((1,lookback))\n",
    "        avg_val_loss = 0\n",
    "        c=0\n",
    "        \n",
    "        val_preds = torch.zeros(len(validate_loader))\n",
    "        val_targets = torch.zeros(len(validate_loader))\n",
    "        for k, val in enumerate(validate_loader):\n",
    "            model.eval()\n",
    "            model_out = model(val[:][0].view(-1,lookback,number_of_features))\n",
    "#             print(model_out[0])\n",
    "            attn = model_out[1]\n",
    "            total_attn = np.vstack((total_attn,attn.detach().numpy()))\n",
    "            c+=1\n",
    "#             print(attn.shape)\n",
    "            y_epoch = model_out[0].reshape(-1)\n",
    "            loss_epoch = criterion(y_epoch,val[:][1])\n",
    "            avg_val_loss += loss_epoch.item() #/ len(validate_loader)\n",
    "            val_preds = torch.cat((val_preds, y_epoch), 0)\n",
    "            val_targets = torch.cat((val_targets, val[:][1]), 0)\n",
    "            \n",
    "        print(\"Validation metrics  : Loss\",avg_val_loss, validate_me(val_targets,val_preds))\n",
    "        print(\"Runs: \",c)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5fa830",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_tar))\n",
    "print(len(train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_pred>0.).detach().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e861fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum((train_tar==train_pred).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d83178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running predictions on train dataset to obtain attention weights\n",
    "\n",
    "test_set = timeseries( bal_train_rsd, bal_train_target)\n",
    "test_loader = DataLoader(test_set,shuffle=False,batch_size=batch_size)\n",
    "\n",
    "\n",
    "test_total_attn = np.zeros((1,lookback))\n",
    "avg_test_loss = 0\n",
    "c=0\n",
    "\n",
    "test_preds = torch.zeros(len(test_loader))\n",
    "test_targets = torch.zeros(len(test_loader))\n",
    "for k, val in enumerate(test_loader):\n",
    "    model.eval()\n",
    "    model_out = model(val[:][0].view(-1,lookback,number_of_features))\n",
    "#             print(model_out[0])\n",
    "    attn = model_out[1]\n",
    "    test_total_attn = np.vstack((test_total_attn,attn.detach().numpy()))\n",
    "    c+=1\n",
    "#             print(attn.shape)\n",
    "    y_epoch = model_out[0].reshape(-1)\n",
    "    loss_epoch = criterion(y_epoch,val[:][1])\n",
    "    avg_test_loss += loss_epoch.item() #/ len(validate_loader)\n",
    "    test_preds = torch.cat((test_preds, y_epoch), 0)\n",
    "    test_targets = torch.cat((test_targets, val[:][1]), 0)\n",
    "#             val_preds.append(y_epoch)\n",
    "#             val_targets.append(val[:][1])\n",
    "\n",
    "# print(\"Candidate pool metrics  : Loss\",avg_val_loss, validate_me(val_targets,val_preds))\n",
    "# print(\"Runs: \",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_total_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d521335",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_df = pd.DataFrame(data=total_attn[1:])    # values\n",
    "# ...              index=data[1:,0],    # 1st column as index\n",
    "# ...              columns=data[0,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a54b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bal_validate_data = validate_data[validate_data['SessionID'].isin(bal_validate_ids)]\n",
    "\n",
    "validate_pred_attn = pd.DataFrame({'SessionID':bal_validate_ids , 'Predicted_Flag': val_preds.detach().numpy()[:len(bal_validate_ids)]})\n",
    "print(validate_pred_attn.shape)\n",
    "validate_pred_attn = pd.concat([validate_pred_attn, attn_df],axis=1)\n",
    "print(validate_pred_attn.shape)\n",
    "#, 'Attention_Wt': total_attn[1:] })\n",
    "# test_clusters = pd.DataFrame({'SessionID': validate_cl.reset_index().SessionID , 'Cluster': val_clusters})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75510ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validate_pred_attn.SessionID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb794d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_attn[len(bal_train_ids)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3653af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attn_df = pd.DataFrame(data=test_total_attn[1:len(bal_train_ids)+1]) \n",
    "\n",
    "test_pred_attn = pd.DataFrame({'SessionID':bal_train_ids , 'Predicted_Flag': test_preds.detach().numpy()[:len(bal_train_ids)]})\n",
    "\n",
    "# test_attn_df = pd.DataFrame(data=train_total_attn[1:len(bal_train_ids)+1]) \n",
    "\n",
    "# test_pred_attn = pd.DataFrame({'SessionID':bal_train_ids , 'Predicted_Flag': train_pred.detach().numpy()[:len(bal_train_ids)]})\n",
    "\n",
    "print(test_pred_attn.shape)\n",
    "\n",
    "\n",
    "\n",
    "test_pred_attn = pd.concat([test_pred_attn, test_attn_df],axis=1)\n",
    "print(test_pred_attn.shape)\n",
    "print(len(test_pred_attn.SessionID.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_attn.describe()\n",
    "test_pred_attn['Adj_Predicted_Flag'] = (test_pred_attn['Predicted_Flag']>0.).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca165e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(test_pred_attn['Adj_Predicted_Flag'] )\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64cfc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(test_pred_attn['Adj_Predicted_Flag']==bal_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e89e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_test_pred_attn = test_pred_attn[test_pred_attn['Adj_Predicted_Flag']==bal_train_target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857314b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corr_test_pred_attn)/len(test_pred_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bal_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbae20a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adj_train_pred=(train_pred>0.).detach().numpy().astype(int)\n",
    "print(sum((train_tar.detach().numpy()==adj_train_pred))/len(adj_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a91c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_pred_attn[validate_pred_attn['Predicted_Flag']>1].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70248f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a27c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_pred_attn[validate_pred_attn['Predicted_Flag']==0.].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b9f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_pred_attn[validate_pred_attn['SessionID']==10003020]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e9d7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "import time\n",
    "st = time.time()\n",
    "\n",
    "a = torch.flatten(torch.tensor(test_rsd[0:10]), start_dim=1)  #simulates running traces\n",
    "b = torch.flatten(torch.tensor(validate_rsd), start_dim=1) #dataset to find candidates from, also used for tuning the model \n",
    "\n",
    "print(torch.cdist(a, b).topk(100, largest=False)[1][0:10])\n",
    "\n",
    "print( time.time() - st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8ca88",
   "metadata": {},
   "source": [
    "def get_topk_ns(running_traces, k):\n",
    "\n",
    "    a = torch.flatten(torch.tensor(running_traces), start_dim=1)\n",
    "    b = torch.flatten(torch.tensor(validate_rsd), start_dim=1)\n",
    "    nn_test = torch.cdist(a, b).topk(k, largest=False)[1]\n",
    "#     print(torch.cdist(a, b).topk(100)[1][0:10])\n",
    "    return(nn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fae3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = session_wrk_lkbk[['SessionID','TIMESTAMP','PAGE_NAME', 'AgeCategory', 'Gender','Flag']]\n",
    "\n",
    "np.random.seed(420)\n",
    "\n",
    "\n",
    "nn_test = nd[nd['SessionID'].isin(np.random.choice(bal_train_ids,10000))]\n",
    "\n",
    "\n",
    "nn_validate = nd[nd['SessionID'].isin(np.random.choice(bal_validate_ids,250))]\n",
    "\n",
    "\n",
    "nn_eval = nd[nd['SessionID'].isin(np.random.choice(bal_test_ids,1000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a133520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pre-process eval set - WINDOWSSS\n",
    "# min_los= lookback/3\n",
    "min_los = 3\n",
    "uni_ids = nn_eval.SessionID.unique()\n",
    "nn_eval_pp = pd.DataFrame()\n",
    "\n",
    "for id in uni_ids:\n",
    "    id_sub = nn_eval[nn_eval.SessionID == id]\n",
    "    los = id_sub.shape[0]\n",
    "#     print(id_sub.shape)\n",
    "    for i in np.arange(min_los,los-1,1):\n",
    "        id_sub.loc[:,'SessionID'] = str(id)+'_'+str(int(i))\n",
    "#         print(i)\n",
    "        sub = id_sub.iloc[0:int(i),:]\n",
    "        nn_eval_pp = nn_eval_pp.append(sub)\n",
    "        \n",
    "# nn_eval_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e10ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nn_eval_pp.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn_test.shape, nn_test.Flag.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random\n",
    "\n",
    "stime= time.time()\n",
    "\n",
    "running_traces = nn_validate.copy()\n",
    "# running_traces = nn_validate[nn_validate['Flag']==1]\n",
    "# min_los = int(lookback/2)\n",
    "min_los = 3\n",
    "idx_to_drop = []\n",
    "\n",
    "case_ids = running_traces.SessionID.unique()\n",
    "\n",
    "for id in case_ids:\n",
    "    case_level = running_traces.loc[running_traces['SessionID']==id,:]\n",
    "    k_max = case_level.shape[0]\n",
    "\n",
    "    k = random.randint(min_los, k_max)  #random K prefix\n",
    "#     print(case_level.index.tolist()[k:])\n",
    "    idx_to_drop.extend(case_level.index.tolist()[k:])\n",
    "    \n",
    "running_traces=running_traces.drop(idx_to_drop)\n",
    "\n",
    "print(time.time()-stime)\n",
    "print(running_traces.shape, nn_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(case_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc658df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time, random\n",
    "\n",
    "stime= time.time()\n",
    "\n",
    "candidate_pool = nn_test.copy()\n",
    "# min_los = int(lookback/2)\n",
    "min_los = 3\n",
    "idx_to_drop = []\n",
    "\n",
    "case_ids = candidate_pool.SessionID.unique()\n",
    "\n",
    "for id in case_ids:\n",
    "    case_level = candidate_pool.loc[candidate_pool['SessionID']==id,:]\n",
    "    tlos = case_level.shape[0]\n",
    "\n",
    "    k = tlos-1\n",
    "#     print(case_level.index.tolist()[k:])\n",
    "    idx_to_drop.extend(case_level.index.tolist()[k:])  # n-1 prefix\n",
    "    \n",
    "candidate_pool=candidate_pool.drop(idx_to_drop)\n",
    "\n",
    "print(time.time()-stime)\n",
    "print(candidate_pool.shape, nn_validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88d814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.util.constants import PARAMETER_CONSTANT_ACTIVITY_KEY\n",
    "\n",
    "# nd = session_wrk_lkbk[['SessionID','TIMESTAMP','PAGE_NAME']]\n",
    "# nn_test = nd[nd['SessionID'].isin(test_ids[0:4])]\n",
    "\n",
    "# # log_csv = dataframe_utils.convert_timestamp_columns_in_df(nn_sub)\n",
    "# log_csv = nn_sub.sort_values('TIMESTAMP')\n",
    "\n",
    "# parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'SessionID'}\n",
    "\n",
    "# event_log = log_converter.apply(log_csv, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "\n",
    "# print(event_log)\n",
    "\n",
    "# from pm4py.algo.conformance.alignments.edit_distance import algorithm as logs_alignments\n",
    "# parameters = {}\n",
    "# alignments = logs_alignments.apply(event_log[0], event_log[1], parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "from pm4py.objects.log.obj import EventLog\n",
    "\n",
    "def convert_df_to_EventLogObj(df, case_id, activity_key, timestamp_key):\n",
    "    log = pm4py.format_dataframe(df,\n",
    "                                 case_id = case_id,\n",
    "                                 activity_key= activity_key,\n",
    "                                 timestamp_key= timestamp_key)\n",
    "\n",
    "    log = pm4py.convert_to_event_log(log)\n",
    "    \n",
    "    trace = EventLog()\n",
    "    for i in range(1,len(log)):\n",
    "        trace.append(log[i])\n",
    "    \n",
    "    return trace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19617076",
   "metadata": {},
   "source": [
    "running_traces[running_traces['SessionID']==10169326]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_trace_obj = convert_df_to_EventLogObj(running_traces, case_id = \"SessionID\",\n",
    "                                     activity_key= \"PAGE_NAME\",\n",
    "                                     timestamp_key= \"TIMESTAMP\")\n",
    "candidate_pool_obj = convert_df_to_EventLogObj(candidate_pool, case_id = \"SessionID\",\n",
    "                                     activity_key= \"PAGE_NAME\",\n",
    "                                     timestamp_key= \"TIMESTAMP\")\n",
    "\n",
    "eval_set_obj = convert_df_to_EventLogObj(nn_eval_pp, case_id = \"SessionID\",\n",
    "                                     activity_key= \"PAGE_NAME\",\n",
    "                                     timestamp_key= \"TIMESTAMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c74015",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_trace_obj[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pool_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b1a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running_trace_obj\n",
    "from pm4py.objects.log.util import log_regex\n",
    "\n",
    "# form a mapping dictionary associating each activity of the two logs to an ASCII character\n",
    "mapping = log_regex.form_encoding_dictio_from_two_logs(running_trace_obj, candidate_pool_obj, parameters={})\n",
    "# encode the second log (against which we want to align each trace of the first log)\n",
    "list_encodings = log_regex.get_encoded_log(candidate_pool_obj, mapping, parameters={})   #ASCII encoding for logs in search space for candidate selection\n",
    "# # optimization: keep one item per variant\n",
    "set_encodings = set(list_encodings) # set absrraction\n",
    "list_encodings = list(set_encodings) #list abstraction\n",
    "\n",
    "list_encodings = sorted(list_encodings, key=lambda x: len(x)) #sorting the list by length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_pool_seq = {}\n",
    "\n",
    "for trace in candidate_pool_obj:\n",
    "    cand_pool_seq[trace._get_attributes()['concept:name']] = log_regex.get_encoded_trace(trace, mapping, parameters={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b91f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set_seq = {}\n",
    "\n",
    "for trace in eval_set_obj:\n",
    "    eval_set_seq[trace._get_attributes()['concept:name']] = log_regex.get_encoded_trace(trace, mapping, parameters={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b905323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval_set_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6943e7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "cand_pool_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cand_pool_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce899a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stringdist\n",
    "\n",
    "def get_closest_traces(encoded_trace, list_encodings, dist_thresh):\n",
    "#     dist_thresh = 0.4\n",
    "    closest_trace = list()\n",
    "    c=-1\n",
    "    for i in range(0, len(list_encodings)):\n",
    "    #     print(len(list_encodings[i]))\n",
    "        dist = (stringdist.levenshtein_norm( encoded_trace, list_encodings[i]))\n",
    "    #     dist = (stringdist.levenshtein( encoded_trace, list_encodings[i]))\n",
    "    #     if c == -1:\n",
    "    #         min_dist=dist\n",
    "        if dist <= dist_thresh:\n",
    "            min_dist=dist\n",
    "            closest_trace.append(list_encodings[i])\n",
    "    #         closest_trace.append(log_regex.get_encoded_log(list_encodings[i], mapping, parameters={}))\n",
    "            c += 1\n",
    "#         elif dist == min_dist:\n",
    "#             closest_trace.append(list_encodings[i])\n",
    "#             c += 1\n",
    "\n",
    "\n",
    "#     print(min_dist,c)\n",
    "    return(closest_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_pred_attn.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ef0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(session_wrk_lkbk[session_wrk_lkbk['SessionID']==55292373]['PAGE_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ba120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_pred_attn.iloc[:,2:].sum(axis=1)\n",
    "validate_pred_attn.loc[validate_pred_attn['SessionID']==55292373,:].iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from weighted_levenshtein import lev, osa, dam_lev\n",
    "\n",
    "def get_attn_wts(rt, idc, cand_id):\n",
    "#     print(idc)\n",
    "    runn_attn_wts = validate_pred_attn.loc[validate_pred_attn['SessionID']==(int(idc)),:].iloc[:,2:]\n",
    "    cand_attn_wts = test_pred_attn.loc[test_pred_attn['SessionID']==(int(cand_id)),:].iloc[:,2:]\n",
    "#     print(attn_wts)\n",
    "    f = validate_pred_attn.loc[validate_pred_attn['SessionID']==(int(idc)),'Predicted_Flag']\n",
    "    # print(f.any())\n",
    "    pred_flag = int(f.any())\n",
    "    # print(pred_flag)\n",
    "\n",
    "    sign_mul = 1 if pred_flag==0 else -1\n",
    "#     print(sign_mul)\n",
    "    n=9\n",
    "#     print(cand_pool_seq[str(idc)])\n",
    "    los = len(cand_pool_seq[str(cand_id)])\n",
    "\n",
    "    substitute_costs = np.ones((128, 128), dtype=np.float64)\n",
    "\n",
    "    for i in np.arange(los,0,-1):\n",
    "        \n",
    "        curr_acc = cand_pool_seq[str(cand_id)][i-1]\n",
    "#         print(len(cand_attn_wts.loc[:,:]), i)\n",
    "        attwt = (cand_attn_wts.iloc[:,i-1])\n",
    "        if attwt.any() > float(1/lookback):\n",
    "            curr_acc_cost = (attwt) - 1\n",
    "    #         print(curr_acc_cost)\n",
    "            substitute_costs[ord(curr_acc), :] = curr_acc_cost\n",
    "    \n",
    "    los = len(rt)\n",
    "    \n",
    "    for i in np.arange(los,0,-1):\n",
    "        \n",
    "        curr_acc = rt[i-1]\n",
    "        attwt = (runn_attn_wts.iloc[:,i-1])\n",
    "        if attwt.any() > float(1/lookback):\n",
    "            curr_acc_cost = (attwt ) + 1\n",
    "    #         print(curr_acc_cost)\n",
    "            substitute_costs[:, ord(curr_acc)] = curr_acc_cost\n",
    "\n",
    "    return substitute_costs\n",
    "#     print(curr_acc_cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stringdist\n",
    "\n",
    "def get_closest_traces_attn(encoded_trace, trace_id, dist_thresh):\n",
    "#     dist_thresh = 0.4\n",
    "    closest_trace = list()\n",
    "    c=-1\n",
    "#     sub_costs = get_attn_wts(trace_id)\n",
    "    word = encoded_trace\n",
    "    for i in cand_pool_seq:\n",
    "#         print(i)\n",
    "        sub_costs = get_attn_wts(word, trace_id, i)\n",
    "        word1 = cand_pool_seq[i]\n",
    "        dist = lev(word,word1, substitute_costs=sub_costs)/max(len(word),len(word1))\n",
    "    #     print(len(list_encodings[i]))\n",
    "#         dist = (stringdist.levenshtein_norm( encoded_trace, list_encodings[i]))\n",
    "        if dist <= dist_thresh:\n",
    "            min_dist=dist\n",
    "            closest_trace.append(i)\n",
    "            c += 1\n",
    "    return(closest_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_traces(encoded_trace, trace_id, dist_thresh):\n",
    "#     dist_thresh = 0.4\n",
    "    closest_trace = list()\n",
    "    c=-1\n",
    "#     sub_costs = get_attn_wts(trace_id)\n",
    "    word = encoded_trace\n",
    "    for i in eval_set_seq:\n",
    "#         print(i)\n",
    "#         sub_costs = get_attn_wts(word, trace_id, i)\n",
    "        word1 = eval_set_seq[i]\n",
    "        dist = lev(word,word1)/max(len(word),len(word1))\n",
    "    #     print(len(list_encodings[i]))\n",
    "#         dist = (stringdist.levenshtein_norm( encoded_trace, list_encodings[i]))\n",
    "        if dist <= dist_thresh:\n",
    "            min_dist=dist\n",
    "            closest_trace.append(i)\n",
    "            c += 1\n",
    "    return(closest_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a489cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dist_thresh = 0.5\n",
    "eval_dist_thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ee1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "st=time.time()\n",
    "\n",
    "num_matches = np.zeros(len(running_trace_obj))\n",
    "num_seqmatches = np.zeros(len(running_trace_obj))\n",
    "set_of_close_trace = {}\n",
    "e=0\n",
    "i=0\n",
    "ids_for_close_trace = {}\n",
    "\n",
    "ids_for_Eval_trace = {}\n",
    "\n",
    "for trace in running_trace_obj:\n",
    "#     print(trace)\n",
    "    encoded_trace = log_regex.get_encoded_trace(trace, mapping, parameters={}) #ASCII encoding for running trace\n",
    "    trace_id = trace._get_attributes()['concept:name']\n",
    "    \n",
    "    closest_trace_ids = get_closest_traces_attn(encoded_trace, trace_id, sim_dist_thresh)\n",
    "    \n",
    "    eval_trace_ids = get_eval_traces(encoded_trace, trace_id, eval_dist_thresh)\n",
    "    ids_for_Eval_trace[trace_id] = eval_trace_ids\n",
    "    if len(closest_trace_ids) == 0:\n",
    "        e+=1\n",
    "#         closest_traces = get_closest_traces(encoded_trace, list_encodings, 0.4)\n",
    "    ids_for_close_trace[trace_id] = closest_trace_ids\n",
    "\n",
    "    num_matches[i] = len(closest_trace_ids) \n",
    "    i+=1\n",
    "    \n",
    "print(time.time()-st)\n",
    "\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.util import string_distance\n",
    "import difflib\n",
    "from pm4py.objects.log.util import log_regex\n",
    "\n",
    "num_matches = np.zeros(len(running_trace_obj))\n",
    "num_seqmatches = np.zeros(len(running_trace_obj))\n",
    "set_of_close_trace = {}\n",
    "e=0\n",
    "i=0\n",
    "for trace in running_trace_obj:\n",
    "#     print(trace)\n",
    "    encoded_trace = log_regex.get_encoded_trace(trace, mapping, parameters={}) #ASCII encoding for running trace\n",
    "    closest_traces = get_closest_traces(encoded_trace, list_encodings, sim_dist_thresh)\n",
    "    if len(closest_traces) == 0:\n",
    "        e+=1\n",
    "#         closest_traces = get_closest_traces(encoded_trace, list_encodings, 0.4)\n",
    "    set_of_close_trace[trace._get_attributes()['concept:name']] = closest_traces\n",
    "#     if encoded_trace in set_encodings:  #if exact match is found\n",
    "#         argmin_dist = encoded_trace\n",
    "# #         print(\"Found\")\n",
    "#     else:\n",
    "#         argmin_dist = string_distance.argmin_levenshtein(encoded_trace, list_encodings)  #returns closest string\n",
    "    #     print(argmin_dist)\n",
    "    c=0\n",
    "#     for tr in closest_traces:\n",
    "#         seq_match = difflib.SequenceMatcher(None, encoded_trace, tr).get_matching_blocks()  #finds matching sequences in closest string\n",
    "#         c += (len(seq_match) - 1)\n",
    "#     num_seqmatches[i] = c\n",
    "    num_matches[i] = len(closest_traces) \n",
    "    i+=1\n",
    "    \n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "ids_for_close_trace0 = {}\n",
    "for k in set_of_close_trace:\n",
    "    ids_for_close_trace0[k]=[]\n",
    "    for seq in set_of_close_trace[k]:\n",
    "        for k2 in cand_pool_seq:\n",
    "            if seq==cand_pool_seq[k2]:\n",
    "                ids_for_close_trace0[k].append(k2.rstrip())\n",
    "                \n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6280403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_prefix(prefix, los, n):\n",
    "    if len(prefix.split('-')) <= n:\n",
    "        return prefix\n",
    "    else:\n",
    "        split_prefix = prefix.split('-')[1:]\n",
    "        pref='-'.join(str(e) for e in split_prefix)\n",
    "        return pref\n",
    "            \n",
    "def update_los(pref,los,n):\n",
    "    if len(pref.split('-'))>n:\n",
    "        return los\n",
    "    else:\n",
    "        return los+1\n",
    "\n",
    "def create_transys_n(sess_page_level,n):\n",
    "    transys = {}\n",
    "    unique_sess =  sess_page_level['SessionID'].unique()\n",
    "    sess_feat_row = 0\n",
    "    los = 0\n",
    "    for sess in unique_sess:\n",
    "#         sess_feat.at[sess_feat_row,'SessionID'] = sess\n",
    "        session_level = sess_page_level.loc[sess_page_level['SessionID']==sess]\n",
    "#         print(session_level.shape[0])\n",
    "        first_page=1\n",
    "        c = 0\n",
    "        prefix = \"SOS\"\n",
    "        prev=\"\"\n",
    "        for index,row in session_level.iterrows():\n",
    "            c +=1\n",
    "            \n",
    "            prefix = pop_prefix(prefix, los, n)\n",
    "#             print(prefix)\n",
    "            curr = session_level.at[index,'PAGE_NAME']\n",
    "            if first_page==1:\n",
    "                first_page=0\n",
    "#                 prefix.append(curr)\n",
    "#             if prev!=curr:\n",
    "                prefix = prefix + '-' + curr\n",
    "#                 print(\"1\", prev, curr, prefix)\n",
    "                los = 1\n",
    "#                     los += 1\n",
    "                prev=curr\n",
    "#                 print(prefix)\n",
    "                \n",
    "#                 continue\n",
    "            else:\n",
    "#                 print(prev,curr)\n",
    "                if prev!=curr:\n",
    "                    if prefix in transys.keys() and curr not in transys.get(prefix):\n",
    "                        \n",
    "                        transys.setdefault(prefix, []).append(curr)\n",
    "#                         print(str(transys))\n",
    "#                 prefix.append(curr)\n",
    "                        prefix = prefix + '-' + curr\n",
    "#                         print(\"2\", prev, curr, prefix)\n",
    "#                         los = update_los(prefix,los,n)\n",
    "                        los += 1\n",
    "                    elif prefix not in transys.keys():\n",
    "#                         prefix = pop_prefix(prefix, los, n)\n",
    "                        transys.setdefault(prefix, []).append(curr)\n",
    "                        prefix = prefix + '-' + curr\n",
    "#                         print(\"3\", prev, curr, prefix)\n",
    "#                         los = update_los(prefix,los,n)\n",
    "                        los += 1\n",
    "#                         print(str(transys))\n",
    "                    prev=curr\n",
    "#                     print(\"Adding\")\n",
    "                else:\n",
    "                    if prefix not in transys.keys():\n",
    "                        transys.setdefault(prefix, []).append(curr)\n",
    "                    elif curr not in transys.get(prefix):\n",
    "                        transys.setdefault(prefix, []).append(curr)\n",
    "#             if c==session_level.shape[0]:\n",
    "#                 prefix = pop_prefix(prefix, los, n)\n",
    "#                 if prefix in transys.keys() and \"EOS\" not in transys.get(prefix):\n",
    "#                     transys.setdefault(prefix, []).append(\"EOS\")\n",
    "#                 elif prefix not in transys.keys():\n",
    "#                     transys.setdefault(prefix, []).append(\"EOS\")\n",
    "#                     print(\"Skipping\")\n",
    "    \n",
    "#     bigram_df = pd.DataFrame(vect.fit_transform(sess_feat['twograms'].values).todense(), columns = vect.get_feature_names())\n",
    "\n",
    "#     final_df = pd.concat([sess_feat,bigram_df], axis=1)\n",
    "    return(transys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638333a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ids_for_close_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93600c",
   "metadata": {
    "tags": []
   },
   "source": [
    "l1 = set(ids_for_close_trace['10668376'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce1494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ids_for_Eval_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055cd8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats import weightstats as stests\n",
    "\n",
    "c=0\n",
    "e=0\n",
    "npk = 0 #No key in positive outcomes subset\n",
    "nnk = 0 #No key in negative outcomes subset\n",
    "nra = 0 #No reccomended activities\n",
    "knf = 0 #Running trace last state not found in eval TS\n",
    "nevtr = 0 #No evaluation traces ffound\n",
    "\n",
    "tot_cnt=0\n",
    "pos_trc_cnt=0\n",
    "pos_rec_cnt=0\n",
    "neg_rec_cnt=0\n",
    "\n",
    "valid_rec_cnt=0\n",
    "\n",
    "ev_trc_cnt=0\n",
    "pos_sub_cnt=0\n",
    "neg_sub_cnt=0\n",
    "\n",
    "pos_sub_imp=0\n",
    "pos_sub_wrs=0\n",
    "pos_sub_imp_sg=0\n",
    "pos_sub_wrs_sg=0\n",
    "\n",
    "results = {}\n",
    "\n",
    "pot_recs_for_running_tr = {}\n",
    "\n",
    "id_for_recofoll = {}\n",
    "id_for_reconfoll = {}\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for id in ids_for_close_trace:\n",
    "    tot_cnt+=1\n",
    "    l1 = set(ids_for_close_trace[str(id)])\n",
    "    l2 = set(ids_for_close_trace0[str(id)])\n",
    "    if(len(l1)==0):\n",
    "        e+=1\n",
    "    if(l1==l2):\n",
    "#         print(l1, l2)\n",
    "        c+=1\n",
    "#     print(id)\n",
    "    if(len(l1)!=0):\n",
    "        pos_trc_cnt+=1\n",
    "        \n",
    "    #     print(running_traces[running_traces['SessionID']==id])\n",
    "        #get state of running trace\n",
    "    #     print(create_transys_n(running_traces[running_traces['SessionID']==int(id)].sort_values(['TIMESTAMP']),2).keys())\n",
    "        rt_state = list(create_transys_n(running_traces[running_traces['SessionID']==int(id)].sort_values(['TIMESTAMP']),2).keys())[-1]\n",
    "        #get transition system from similar traces\n",
    "#         print(list(l1))\n",
    "        closest_df = nn_test[nn_test['SessionID'].isin(list(map(int,l1)))]\n",
    "#         print(closest_df)\n",
    "        closest_df_pos = closest_df[closest_df['Flag']==0]\n",
    "        closest_df_neg = closest_df[closest_df['Flag']==1]\n",
    "\n",
    "        ts_closest_df_pos= create_transys_n(closest_df_pos.sort_values(['SessionID','TIMESTAMP']),2)\n",
    "        ts_closest_df_neg= create_transys_n(closest_df_neg.sort_values(['SessionID','TIMESTAMP']),2)\n",
    "        \n",
    "#         pos_rec\n",
    "#         neg_rec\n",
    "\n",
    "#         print(rt_state)\n",
    "        reco = []\n",
    "        pos_rec=[]\n",
    "        neg_rec=[]\n",
    "    \n",
    "#         print(ts_closest_df_pos)\n",
    "#         print(ts_closest_df_neg)\n",
    "        f0 = rt_state not in list(ts_closest_df_pos.keys())\n",
    "        f1 =  rt_state not in list(ts_closest_df_neg.keys())\n",
    "        f2 = 0\n",
    "        if f0:\n",
    "            npk += 1\n",
    "#             print(\"No pos pot recos\")\n",
    "#             reco = []\n",
    "        else:\n",
    "            pos_rec_cnt+=1\n",
    "            pos_rec = ts_closest_df_pos[rt_state]\n",
    "#             print(\"Pos out potential recos :\", pos_rec)\n",
    "        if f1:\n",
    "            nnk += 1\n",
    "#             print(\"No neg pot recos\")\n",
    "            reco=pos_rec\n",
    "            f2=1\n",
    "        else:\n",
    "            neg_rec_cnt+=1\n",
    "            neg_rec = ts_closest_df_neg[rt_state]\n",
    "#             print(\"Neg out potential recos :\", neg_rec)\n",
    "#         print(ts_closest_df_pos[rt_state], ts_closest_df_neg[rt_state])\n",
    "        \n",
    "#         if(len(neg_rec)==0):\n",
    "#                 print(\"No neg pot recos\")\n",
    "#                 reco=pos_rec\n",
    "#         else:\n",
    "        if not f0 and not f1 and not f2:\n",
    "            for acc in pos_rec:\n",
    "                if acc not in neg_rec:\n",
    "                    reco.append(acc)\n",
    "                    \n",
    "        if len(reco)>0:\n",
    "            valid_rec_cnt+=1\n",
    "            pot_recs_for_running_tr[id]=reco\n",
    "#             print(\"Recommended activity\", reco)\n",
    "            \n",
    "            pos_reco_foll=0\n",
    "            pos_reco_nfoll=0\n",
    "            \n",
    "            \n",
    "            \n",
    "            tot = len(ids_for_Eval_trace[id])\n",
    "            \n",
    "            if tot>0:\n",
    "                ev_trc_cnt+=1\n",
    "#                 print(tot)\n",
    "                eval_trace_ids = ids_for_Eval_trace[id]\n",
    "                foll_ids=[]\n",
    "                nfoll_ids=[]\n",
    "                for eval_id in eval_trace_ids:\n",
    "#                     print(id, eval_id)\n",
    "                    df = nn_eval_pp.loc[nn_eval_pp['SessionID']==(eval_id),['SessionID','TIMESTAMP','PAGE_NAME']]\n",
    "#                     print(df.shape)\n",
    "                    eval_trace_ts = create_transys_n(df.sort_values(['SessionID','TIMESTAMP']),2)\n",
    "                    if rt_state in eval_trace_ts.keys():\n",
    "                        next_eve = eval_trace_ts[rt_state]\n",
    "#                         print(next_eve, reco)\n",
    "                        if len(set(next_eve).intersection(set(reco)))>0:\n",
    "                            pos_reco_foll+=1\n",
    "                            foll_ids.append(eval_id)\n",
    "                        else:\n",
    "                            pos_reco_nfoll+=1\n",
    "                            nfoll_ids.append(eval_id)\n",
    "                    \n",
    "                    else: \n",
    "                        nfoll_ids.append(eval_id)\n",
    "                        pos_reco_nfoll+=1\n",
    "                        knf+=1\n",
    "#                 tot = pos_reco_foll+pos_reco_nfoll\n",
    "#                 if tot>0:\n",
    "                id_for_recofoll[id]= foll_ids\n",
    "                id_for_reconfoll[id]=  nfoll_ids  \n",
    "#                 print(\"Eval results (Total -- Recco foll -- Recco not followed):\", tot, pos_reco_foll/tot, pos_reco_nfoll/tot)\n",
    "                results[id]=(tot, pos_reco_foll/tot, pos_reco_nfoll/tot)\n",
    "                if pos_reco_foll!=0:\n",
    "                    pos_sub_cnt+=1\n",
    "                    a = nn_eval_pp[nn_eval_pp['SessionID'].isin(foll_ids)]['Flag']\n",
    "#                     print(sum(nn_eval_pp[nn_eval_pp['SessionID'].isin(foll_ids)]['Flag'])/len((nn_eval_pp[nn_eval_pp['SessionID'].isin(foll_ids)]['Flag'])))\n",
    "                if pos_reco_nfoll!=0:\n",
    "                    neg_sub_cnt+=1\n",
    "                    b = nn_eval_pp[nn_eval_pp['SessionID'].isin(nfoll_ids)]['Flag']\n",
    "#                     print(sum(nn_eval_pp[nn_eval_pp['SessionID'].isin(nfoll_ids)]['Flag'])/len((nn_eval_pp[nn_eval_pp['SessionID'].isin(nfoll_ids)]['Flag'])))\n",
    "                if pos_reco_foll!=0 and pos_reco_nfoll!=0:\n",
    "                    \n",
    "                    ztest ,pval1 = stests.ztest(a, x2=b, value=0,alternative='two-sided')\n",
    "#                     print(\"P-value: \",float(pval1))\n",
    "                    if(np.mean(a)/np.mean(b)>1):\n",
    "                        pos_sub_imp+=1\n",
    "                        if(pval1 < 0.05):\n",
    "                            pos_sub_imp_sg+=1\n",
    "                    else:\n",
    "                        pos_sub_wrs+=1\n",
    "                        if(pval1 < 0.05):\n",
    "                            pos_sub_wrs_sg+=1\n",
    "                        \n",
    "#                         print(\"Stat sign.\")\n",
    "            else:\n",
    "                nevtr+=1\n",
    "#             eval_set_seq\n",
    "        else:\n",
    "            nra+=1\n",
    "#             print(\"No reco!\")\n",
    "            \n",
    "print(time.time()-st)\n",
    "    \n",
    "print(e, c, npk, nnk, nra)\n",
    "print(\"Total running traces checked:\", tot_cnt)\n",
    "print(\"Similar traces found for :\", pos_trc_cnt)\n",
    "print(\"Pos outcome recco found for:\", pos_rec_cnt)\n",
    "print(\"Neg outcome recco found for:\", neg_rec_cnt)\n",
    "\n",
    "print(\"Valid recco found for:\",valid_rec_cnt)\n",
    "\n",
    "print(\"Eval traces found for\", ev_trc_cnt)\n",
    "print(\"Pos sub found for\", pos_sub_cnt)\n",
    "print(\"Neg sub found for\", neg_sub_cnt)\n",
    "print(\"Pos sub outcome improvement for\", pos_sub_imp)\n",
    "print(\"Stat sig imp for\", pos_sub_imp_sg)\n",
    "print(\"Pos sub outcome worsens for\", pos_sub_wrs)\n",
    "print(\"Stat sig wrse for\", pos_sub_wrs_sg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004f9ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d645296",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_foll = 0\n",
    "tot_nfoll = 0\n",
    "for g in id_for_recofoll:\n",
    "    tot_foll+=len(id_for_recofoll[g])\n",
    "    \n",
    "for g in id_for_reconfoll:\n",
    "    tot_nfoll+=len(id_for_reconfoll[g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354038f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# id_for_recofoll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats import weightstats as stests\n",
    "\n",
    "c=0\n",
    "e=0\n",
    "npk = 0 #No key in positive outcomes subset\n",
    "nnk = 0 #No key in negative outcomes subset\n",
    "nra = 0 #No reccomended activities\n",
    "knf = 0 #Running trace last state not found in eval TS\n",
    "nevtr = 0 #No evaluation traces ffound\n",
    "\n",
    "tot_cnt=0\n",
    "pos_trc_cnt=0\n",
    "pos_rec_cnt=0\n",
    "neg_rec_cnt=0\n",
    "\n",
    "valid_rec_cnt=0\n",
    "\n",
    "ev_trc_cnt=0\n",
    "pos_sub_cnt=0\n",
    "neg_sub_cnt=0\n",
    "\n",
    "pos_sub_imp=0\n",
    "pos_sub_wrs=0\n",
    "pos_sub_imp_sg=0\n",
    "pos_sub_wrs_sg=0\n",
    "\n",
    "results = {}\n",
    "\n",
    "pot_recs_for_running_tr = {}\n",
    "\n",
    "id_for_recofoll = {}\n",
    "id_for_reconfoll = {}\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for id in ids_for_close_trace:\n",
    "    tot_cnt+=1\n",
    "    l1 = set(ids_for_close_trace[str(id)])\n",
    "    l2 = set(ids_for_close_trace0[str(id)])\n",
    "    if(len(l2)==0):\n",
    "        e+=1\n",
    "    if(l1==l2):\n",
    "#         print(l1, l2)\n",
    "        c+=1\n",
    "#     print(id)\n",
    "    if(len(l2)!=0):\n",
    "        pos_trc_cnt+=1\n",
    "        \n",
    "    #     print(running_traces[running_traces['SessionID']==id])\n",
    "        #get state of running trace\n",
    "    #     print(create_transys_n(running_traces[running_traces['SessionID']==int(id)].sort_values(['TIMESTAMP']),2).keys())\n",
    "        rt_state = list(create_transys_n(running_traces[running_traces['SessionID']==int(id)].sort_values(['TIMESTAMP']),2).keys())[-1]\n",
    "        #get transition system from similar traces\n",
    "#         print(list(l1))\n",
    "        closest_df = nn_test[nn_test['SessionID'].isin(list(map(int,l2)))]\n",
    "#         print(closest_df)\n",
    "        closest_df_pos = closest_df[closest_df['Flag']==0]\n",
    "        closest_df_neg = closest_df[closest_df['Flag']==1]\n",
    "\n",
    "        ts_closest_df_pos= create_transys_n(closest_df_pos.sort_values(['SessionID','TIMESTAMP']),2)\n",
    "        ts_closest_df_neg= create_transys_n(closest_df_neg.sort_values(['SessionID','TIMESTAMP']),2)\n",
    "        \n",
    "#         pos_rec\n",
    "#         neg_rec\n",
    "\n",
    "#         print(rt_state)\n",
    "        reco = []\n",
    "        pos_rec=[]\n",
    "        neg_rec=[]\n",
    "    \n",
    "#         print(ts_closest_df_pos)\n",
    "#         print(ts_closest_df_neg)\n",
    "        f0 = rt_state not in list(ts_closest_df_pos.keys())\n",
    "        f1 =  rt_state not in list(ts_closest_df_neg.keys())\n",
    "        f2 = 0\n",
    "        if f0:\n",
    "            npk += 1\n",
    "#             print(\"No pos pot recos\")\n",
    "#             reco = []\n",
    "        else:\n",
    "            pos_rec_cnt+=1\n",
    "            pos_rec = ts_closest_df_pos[rt_state]\n",
    "#             print(\"Pos out potential recos :\", pos_rec)\n",
    "        if f1:\n",
    "            nnk += 1\n",
    "#             print(\"No neg pot recos\")\n",
    "            reco=pos_rec\n",
    "            f2=1\n",
    "        else:\n",
    "            neg_rec_cnt+=1\n",
    "            neg_rec = ts_closest_df_neg[rt_state]\n",
    "#             print(\"Neg out potential recos :\", neg_rec)\n",
    "#         print(ts_closest_df_pos[rt_state], ts_closest_df_neg[rt_state])\n",
    "        \n",
    "#         if(len(neg_rec)==0):\n",
    "#                 print(\"No neg pot recos\")\n",
    "#                 reco=pos_rec\n",
    "#         else:\n",
    "        if not f0 and not f1 and not f2:\n",
    "            for acc in pos_rec:\n",
    "                if acc not in neg_rec:\n",
    "                    reco.append(acc)\n",
    "                    \n",
    "        if len(reco)>0:\n",
    "            valid_rec_cnt+=1\n",
    "            pot_recs_for_running_tr[id]=reco\n",
    "#             print(\"Recommended activity\", reco)\n",
    "            \n",
    "            pos_reco_foll=0\n",
    "            pos_reco_nfoll=0\n",
    "            \n",
    "            \n",
    "            \n",
    "            tot = len(ids_for_Eval_trace[id])\n",
    "            \n",
    "            if tot>0:\n",
    "                ev_trc_cnt+=1\n",
    "#                 print(tot)\n",
    "                eval_trace_ids = ids_for_Eval_trace[id]\n",
    "                foll_ids=[]\n",
    "                nfoll_ids=[]\n",
    "                for eval_id in eval_trace_ids:\n",
    "#                     print(id, eval_id)\n",
    "                    df = nn_eval_pp.loc[nn_eval_pp['SessionID']==(eval_id),['SessionID','TIMESTAMP','PAGE_NAME']]\n",
    "#                     print(df.shape)\n",
    "                    eval_trace_ts = create_transys_n(df.sort_values(['SessionID','TIMESTAMP']),2)\n",
    "                    if rt_state in eval_trace_ts.keys():\n",
    "                        next_eve = eval_trace_ts[rt_state]\n",
    "#                         print(next_eve, reco)\n",
    "                        if len(set(next_eve).intersection(set(reco)))>0:\n",
    "                            pos_reco_foll+=1\n",
    "                            foll_ids.append(eval_id)\n",
    "                        else:\n",
    "                            pos_reco_nfoll+=1\n",
    "                            nfoll_ids.append(eval_id)\n",
    "                    \n",
    "                    else: \n",
    "                        nfoll_ids.append(eval_id)\n",
    "                        pos_reco_nfoll+=1\n",
    "                        knf+=1\n",
    "#                 tot = pos_reco_foll+pos_reco_nfoll\n",
    "#                 if tot>0:\n",
    "                id_for_recofoll[id]= foll_ids\n",
    "                id_for_reconfoll[id]=  nfoll_ids  \n",
    "#                 print(\"Eval results (Total -- Recco foll -- Recco not followed):\", tot, pos_reco_foll/tot, pos_reco_nfoll/tot)\n",
    "                results[id]=(tot, pos_reco_foll/tot, pos_reco_nfoll/tot)\n",
    "                if pos_reco_foll!=0:\n",
    "                    pos_sub_cnt+=1\n",
    "                    a = nn_eval_pp[nn_eval_pp['SessionID'].isin(foll_ids)]['Flag']\n",
    "#                     print(sum(nn_eval_pp[nn_eval_pp['SessionID'].isin(foll_ids)]['Flag'])/len((nn_eval_pp[nn_eval_pp['SessionID'].isin(foll_ids)]['Flag'])))\n",
    "                if pos_reco_nfoll!=0:\n",
    "                    neg_sub_cnt+=1\n",
    "                    b = nn_eval_pp[nn_eval_pp['SessionID'].isin(nfoll_ids)]['Flag']\n",
    "#                     print(sum(nn_eval_pp[nn_eval_pp['SessionID'].isin(nfoll_ids)]['Flag'])/len((nn_eval_pp[nn_eval_pp['SessionID'].isin(nfoll_ids)]['Flag'])))\n",
    "                if pos_reco_foll!=0 and pos_reco_nfoll!=0:\n",
    "                    \n",
    "                    ztest ,pval1 = stests.ztest(a, x2=b, value=0,alternative='two-sided')\n",
    "#                     print(\"P-value: \",float(pval1))\n",
    "                    if(np.mean(a)/np.mean(b)>1):\n",
    "                        pos_sub_imp+=1\n",
    "                        if(pval1 < 0.05):\n",
    "                            pos_sub_imp_sg+=1\n",
    "                    else:\n",
    "                        pos_sub_wrs+=1\n",
    "                        if(pval1 < 0.05):\n",
    "                            pos_sub_wrs_sg+=1\n",
    "                        \n",
    "#                         print(\"Stat sign.\")\n",
    "            else:\n",
    "                nevtr+=1\n",
    "#             eval_set_seq\n",
    "        else:\n",
    "            nra+=1\n",
    "#             print(\"No reco!\")\n",
    "            \n",
    "print(time.time()-st)\n",
    "    \n",
    "print(e, c, npk, nnk, nra)\n",
    "print(\"Total running traces checked:\", tot_cnt)\n",
    "print(\"Similar traces found for :\", pos_trc_cnt)\n",
    "print(\"Pos outcome recco found for:\", pos_rec_cnt)\n",
    "print(\"Neg outcome recco found for:\", neg_rec_cnt)\n",
    "\n",
    "print(\"Valid recco found for:\",valid_rec_cnt)\n",
    "\n",
    "print(\"Eval traces found for\", ev_trc_cnt)\n",
    "print(\"Pos sub found for\", pos_sub_cnt)\n",
    "print(\"Neg sub found for\", neg_sub_cnt)\n",
    "print(\"Pos sub outcome improvement for\", pos_sub_imp)\n",
    "print(\"Stat sig imp for\", pos_sub_imp_sg)\n",
    "print(\"Pos sub outcome worsens for\", pos_sub_wrs)\n",
    "print(\"Stat sig wrse for\", pos_sub_wrs_sg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
